{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-30T08:14:23.501725Z",
     "start_time": "2025-09-30T08:14:23.479639Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 创建一个 4x4 的图像，批次大小为1，通道数为1\n",
    "x = torch.tensor([[[[ 0.,  1.,  2.,  3.],\n",
    "                   [ 4.,  5.,  6.,  7.],\n",
    "                   [ 8.,  9., 10., 11.],\n",
    "                   [12., 13., 14., 15.]]]])\n",
    "print(\"输入图像形状:\", x.shape) # torch.Size([1, 1, 4, 4])\n",
    "\n",
    "unfold = nn.Unfold(kernel_size=2, stride=2, padding=0)\n",
    "print(unfold(x).reshape(4,4).T)\n",
    "print(unfold(x).shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入图像形状: torch.Size([1, 1, 4, 4])\n",
      "tensor([[ 0.,  1.,  4.,  5.],\n",
      "        [ 2.,  3.,  6.,  7.],\n",
      "        [ 8.,  9., 12., 13.],\n",
      "        [10., 11., 14., 15.]])\n",
      "torch.Size([1, 4, 4])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T08:15:30.462053Z",
     "start_time": "2025-09-30T08:15:30.450813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = F.pad(x, (0, 2, 0, 1, 0, 0))\n",
    "print(x)\n",
    "print(x.shape)"
   ],
   "id": "417160655d39b1b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.,  1.,  2.,  3.,  0.,  0.],\n",
      "          [ 4.,  5.,  6.,  7.,  0.,  0.],\n",
      "          [ 8.,  9., 10., 11.,  0.,  0.],\n",
      "          [12., 13., 14., 15.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.,  0.]]]])\n",
      "torch.Size([1, 1, 5, 6])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:40:28.868624Z",
     "start_time": "2025-09-30T09:40:28.847483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = torch.randn((1,2,4,4))\n",
    "y = F.pad(y, (0, 2, 0, 1,0,0))\n",
    "print(y)\n",
    "print(y.shape)\n",
    "print(y.transpose(1,2).shape)"
   ],
   "id": "8617c788503c9a6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.3838,  2.3252,  1.6461,  0.6313,  0.0000,  0.0000],\n",
      "          [-0.6535, -1.1300,  0.4256, -0.9707,  0.0000,  0.0000],\n",
      "          [-0.3385,  1.9245, -0.5994, -0.8047,  0.0000,  0.0000],\n",
      "          [ 0.0867,  0.1324,  0.2788,  0.2740,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.2247, -1.4819, -0.8830, -0.5641,  0.0000,  0.0000],\n",
      "          [-0.7296, -1.5403, -1.3028, -1.5787,  0.0000,  0.0000],\n",
      "          [-0.2095,  2.0121, -0.3904, -0.2283,  0.0000,  0.0000],\n",
      "          [ 1.4176, -0.7287,  1.1338, -0.1613,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])\n",
      "torch.Size([1, 2, 5, 6])\n",
      "torch.Size([1, 5, 2, 6])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T03:28:54.516988Z",
     "start_time": "2025-10-01T03:28:54.511581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "x = torch.roll(x, (-1, -1), dims=(0,1))\n",
    "print(x)\n",
    "print(x.shape)"
   ],
   "id": "5330f058a3a7ae66",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 6, 4],\n",
      "        [8, 9, 7],\n",
      "        [2, 3, 1]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-30T09:15:00.909317Z",
     "start_time": "2025-09-30T09:15:00.897220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "drop_path_rate = 0.1\n",
    "depths = (2, 2, 6, 2)\n",
    "dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "print(dpr)\n",
    "for i_layer in range(len(depths)):\n",
    "    print(sum(depths[:i_layer]))\n",
    "    drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])]\n",
    "    print(drop_path)"
   ],
   "id": "65cb95fc7be18164",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.00909090880304575, 0.0181818176060915, 0.027272727340459824, 0.036363635212183, 0.045454543083906174, 0.054545458406209946, 0.06363636255264282, 0.0727272778749466, 0.08181818574666977, 0.09090909361839294, 0.10000000149011612]\n",
      "0\n",
      "[0.0, 0.00909090880304575]\n",
      "2\n",
      "[0.0181818176060915, 0.027272727340459824]\n",
      "4\n",
      "[0.036363635212183, 0.045454543083906174, 0.054545458406209946, 0.06363636255264282, 0.0727272778749466, 0.08181818574666977]\n",
      "10\n",
      "[0.09090909361839294, 0.10000000149011612]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T15:29:38.871348Z",
     "start_time": "2025-10-01T15:29:38.861303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([0, 1])  # 假设是x坐标\n",
    "y = torch.tensor([0, 1])     # 假设是y坐标\n",
    "\n",
    "# 默认 indexing='ij'\n",
    "xx_ij, yy_ij = torch.meshgrid(x, y, indexing='ij')\n",
    "a = torch.stack((xx_ij, yy_ij))\n",
    "print(a)\n",
    "a = torch.flatten(a, 1)\n",
    "print(a)\n",
    "a[:, :, None] - a[:, None, :]"
   ],
   "id": "28bd5eb2ac2e843",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0, 0],\n",
      "         [1, 1]],\n",
      "\n",
      "        [[0, 1],\n",
      "         [0, 1]]])\n",
      "tensor([[0, 0, 1, 1],\n",
      "        [0, 1, 0, 1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  0, -1, -1],\n",
       "         [ 0,  0, -1, -1],\n",
       "         [ 1,  1,  0,  0],\n",
       "         [ 1,  1,  0,  0]],\n",
       "\n",
       "        [[ 0, -1,  0, -1],\n",
       "         [ 1,  0,  1,  0],\n",
       "         [ 0, -1,  0, -1],\n",
       "         [ 1,  0,  1,  0]]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-01T16:04:49.268004Z",
     "start_time": "2025-10-01T16:04:49.247131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "print(x[0])"
   ],
   "id": "ed46452c97e324ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CyclicShift(nn.Module):\n",
    "    def __init__(self, displacement):\n",
    "        super().__init__()\n",
    "        self.displacement = displacement\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1, 2))"
   ],
   "id": "897d61a5f1f5fb87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_relative_distances(window_size):\n",
    "    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
    "    distances = indices[None, :, :] - indices[:, None, :]\n",
    "    return distances"
   ],
   "id": "1db08a140f1bc458"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, shifted, window_size, relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        inner_dim = head_dim * heads\n",
    "        self.heads = heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.window_size = window_size\n",
    "        self.relative_pos_embedding = relative_pos_embedding # (13, 13)\n",
    "        self.shifted = shifted\n",
    "\n",
    "        if self.shifted:\n",
    "            displacement = window_size // 2\n",
    "            self.cyclic_shift = CyclicShift(-displacement)\n",
    "            self.cyclic_back_shift = CyclicShift(displacement)\n",
    "            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement, upper_lower=True, left_right=False), requires_grad=False) # (49, 49)\n",
    "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,pper_lower=False, left_right=True), requires_grad=False) # (49, 49)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "        if self.relative_pos_embedding:\n",
    "            self.relative_indices = get_relative_distances(window_size) + window_size - 1\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))\n",
    "        else:\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))\n",
    "\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.shifted:\n",
    "            x = self.cyclic_shift(x)\n",
    "\n",
    "        b, n_h, n_w, _, h = *x.shape, self.heads # [1, 56, 56, _, 3]\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1) # [(1,56,56,96), (1,56,56,96), (1,56,56,96)]\n",
    "        nw_h = n_h // self.window_size # 8\n",
    "        nw_w = n_w // self.window_size # 8\n",
    "        # 分成 h/M * w/M 个窗口\n",
    "        q, k, v = map( lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d', h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n",
    "        # q, k, v : (1, 3, 64, 49, 32)\n",
    "        # 按窗口个数的self-attention\n",
    "        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale # (1,3,64,49,49)\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            dots += self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]]\n",
    "        else:\n",
    "            dots += self.pos_embedding\n",
    "\n",
    "        if self.shifted:\n",
    "            dots[:, :, -nw_w:] += self.upper_lower_mask\n",
    "            dots[:, :, nw_w - 1::nw_w] += self.left_right_mask\n",
    "\n",
    "        attn = dots.softmax(dim=-1) # (1,3,64,49,49)\n",
    "        out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
    "        out = rearrange(out, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)', h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w) # (1, 56, 56, 96) # 窗口合并\n",
    "        out = self.to_out(out)\n",
    "        if self.shifted:\n",
    "            out = self.cyclic_back_shift(out)\n",
    "        return out"
   ],
   "id": "2e39642712b6fb9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class SwinBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, head_dim, mlp_dim, shifted, window_size, relative_pos_embedding):\n",
    "        super().__init__()\n",
    "        self.attention_block = Residual(PreNorm(dim, WindowAttention(dim=dim, heads=heads, head_dim=head_dim, shifted=shifted, window_size=window_size, relative_pos_embedding=relative_pos_embedding)))\n",
    "        self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention_block(x)\n",
    "        x = self.mlp_block(x)\n",
    "        return x"
   ],
   "id": "51f93860fe05c845"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
